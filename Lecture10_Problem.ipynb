{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17cde0c3",
   "metadata": {},
   "source": [
    "# 오늘은 LeNet 구조를 만들어봅시다\n",
    "\n",
    "\n",
    "LeNet 구조는 CNN이며, 초기에 만들어진 모델입니다. \n",
    "\n",
    "2가지 모델(Sigmoid, ReLU)를 만들어 두 모델의 성능을 비교해봅시다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aff3fd",
   "metadata": {},
   "source": [
    "## 1.우선 필요 라이브러리를 import 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd17ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798bac6b",
   "metadata": {},
   "source": [
    "## 2. 딥러닝 모델을 설계할 때 활용하는 장비 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c9880ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.12.1  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5590af",
   "metadata": {},
   "source": [
    "## 3. MNIST 데이터 다운로드 \n",
    "\n",
    " 1. Training data와 Test data 분리하기\n",
    " \n",
    " 2. Training data를 Training data 와 Validation data로 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00908077",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(0.1307, 0.3081), #채우세요),\n",
    "    ])\n",
    "\n",
    "train_data = datasets.MNIST(root='MNIST_data', train=True, download=True, transform=transform)#채우세요\n",
    "test_data = datasets.MNIST(root='MNIST_data', train=False, download=True, transform=transform)\n",
    "#채우세요\n",
    "\n",
    "train, val = torch.utils.data.random_split(train_data, [int(0.8 * len(train_data)), len(train_data) - int(0.8 * len(train_data))]) #채우세요)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True) #채우세요\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE, shuffle=True) #채우세요\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)#채우세요\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437ffb80",
   "metadata": {},
   "source": [
    "## 4. torch.nn을 이용하여 모델-1 만들기\n",
    "\n",
    "   1) 아래의 그림 중 LeNet 구조를 구현 할 것\n",
    "   \n",
    "   2) Sigmoid 활성화 함수를 이용할 것\n",
    "   \n",
    "   \n",
    "![](Comparison_image_neural_networks.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "defacffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Model_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,5, padding=2)\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        self.pool1 = nn.AvgPool2d(2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "        self.pool2 = nn.AvgPool2d(2, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.sigmoid3 = nn.Sigmoid()\n",
    "        \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.sigmoid4 = nn.Sigmoid()\n",
    "        \n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        #채우세요\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.pool1(self.sigmoid1(self.conv1(x)))\n",
    "        x = self.pool2(self.sigmoid2(self.conv2(x)))\n",
    "        \n",
    "        x = x.view(-1, 16*5*5)\n",
    "        \n",
    "        x = self.sigmoid3(self.fc1(x))\n",
    "        x = self.sigmoid4(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        #채우세요\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26eed9",
   "metadata": {},
   "source": [
    "## 5. torch.nn을 이용하여 모델-2 만들기\n",
    "\n",
    "   LeNet 모델에서 ReLU 활성화 함수를 사용하시요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27ac70af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.AvgPool2d(2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.AvgPool2d(2, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        #채우세요\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        \n",
    "        x = x.view(-1, 16*5*5)\n",
    "        \n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.relu4(self.fc2(x))\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        #채우세요\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de556825",
   "metadata": {},
   "source": [
    "## 7. 학습 준비하기\n",
    "\n",
    "1) 1 epoch를 학습할 수 있는 함수 만들기\n",
    "\n",
    "2) Test와 Validation data의 정확도 계산할 수 있는 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06030b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_epoch(train_loader, network, loss_func, optimizer, epoch):\n",
    "    train_losses = []\n",
    "    train_correct = 0\n",
    "    log_interval = 300\n",
    "    \n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image, label = image.to(device), label.to(device)\n",
    "\n",
    "        # 미분값의 초기화\n",
    "        optimizer.zero_grad()\n",
    "        #채우세요\n",
    "\n",
    "        # Forward propagration 계산하기.\n",
    "        outputs = network(image)#채우세요\n",
    "        \n",
    "        \n",
    "        # Cross_entropy 함수를 적용하여 loss를 구하고 저장하기\n",
    "        loss = loss_func(outputs, label)#채우세요\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # training accuracy 정확도 구하기 위해 맞는 샘플 개수 세기\n",
    "        pred = outputs.data.max(1, keepdim=True)[1]#채우세요\n",
    "        train_correct += pred.eq(label.data.view_as(pred)).sum()#채우세요\n",
    "\n",
    "        # Gradinet 구하기\n",
    "        loss.backward()\n",
    "        #채우세요\n",
    "\n",
    "        # weight값 update 하기\n",
    "        optimizer.step()\n",
    "        #채우세요\n",
    "\n",
    "        # 학습 상황 출력\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.2f}%)]\\tLoss: {:.6f}'\n",
    "                  .format(epoch, batch_idx * len(label), len(train_loader.dataset),100. * batch_idx / len(train_loader),\n",
    "                          loss.item()))\n",
    "            \n",
    "    return train_losses, train_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3c0dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_epoch(test_loader, network, loss_func, val = False):\n",
    "    correct = 0\n",
    "    \n",
    "    test_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (image, label) in enumerate(test_loader):\n",
    "            image, label = image.to(device), label.to(device)\n",
    "\n",
    "            # Forward propagration 계산하기.\n",
    "            outputs = network(image)#채우세요\n",
    "\n",
    "            # Cross_entropy 함수를 적용하여 loss를 구하기\n",
    "            loss = loss_func(outputs, label)#채우세요\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "            # Batch 별로 정확도 구하기\n",
    "            pred = outputs.data.max(1, keepdim=True)[1]#채우세요\n",
    "            correct += pred.eq(label.data.view_as(pred)).sum()\n",
    "#채우세요\n",
    "\n",
    "        # 전체 정확도 구하기\n",
    "        test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "        #중간결과 출력\n",
    "        if val is True:\n",
    "                print('Validation set: Accuracy: {}/{} ({:.2f}%)\\n'\n",
    "              .format(correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "        else:\n",
    "            print('Test set: Accuracy: {}/{} ({:.2f}%)\\n'\n",
    "                  .format(correct, len(test_loader.dataset),100. * correct / len(test_loader.dataset)))\n",
    "        \n",
    "    return test_losses, test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d73c53",
   "metadata": {},
   "source": [
    "## 8. 위 정의된 함수로 학습 함수 만들기\n",
    "\n",
    "Adam Optimizer를 사용하여 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df29783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(network, learning_rate = 0.001):\n",
    "    \n",
    "    epoches = 15\n",
    "    \n",
    "    cls_loss = nn.CrossEntropyLoss() #채우세요\n",
    "    optimizer = optim.Adam(network.parameters(), lr=learning_rate)#채우세요\n",
    "    \n",
    "    train_losses_per_epoch = []\n",
    "    test_losses_per_epoch = []\n",
    "    \n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    \n",
    "    for epoch in range(epoches):\n",
    "                \n",
    "        # 모델를 학습 중이라고 선언하기\n",
    "        network.train()\n",
    "        \n",
    "        train_losses, train_correct = training_epoch(train_loader,network,cls_loss,optimizer, epoch)\n",
    "        \n",
    "        # epoch 별로 loss 평균값, 정확도 구하기\n",
    "        average_loss = np.mean(train_losses)\n",
    "        train_losses_per_epoch.append(average_loss)\n",
    "        \n",
    "        train_accuracy = train_correct / len(train_loader.dataset) * 100\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # epoch 별로 정확도 출력\n",
    "        print('\\nTraining set: Accuracy: {}/{} ({:.2f}%)'\n",
    "              .format(train_correct, len(train_loader.dataset),100. * train_correct / len(train_loader.dataset)))\n",
    "\n",
    "        \n",
    "        ### 학습 중에 test 결과 보기\n",
    "        \n",
    "        # 모델 test 중인 것을 선언하기\n",
    "        network.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            test_losses, test_accuracy = test_epoch(val_loader, network, cls_loss, True)\n",
    "\n",
    "        test_losses_per_epoch.append(np.mean(test_losses))\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        test_losses, test_accuracy = test_epoch(test_loader, network, cls_loss, False)\n",
    "        \n",
    "    return train_losses_per_epoch, test_losses_per_epoch, train_accuracies, test_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1394321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/48000 (0.00%)]\tLoss: 2.373318\n",
      "Train Epoch: 0 [19200/48000 (40.00%)]\tLoss: 0.885212\n",
      "Train Epoch: 0 [38400/48000 (80.00%)]\tLoss: 0.315022\n",
      "\n",
      "Training set: Accuracy: 32280/48000 (67.25%)\n",
      "Validation set: Accuracy: 10833/12000 (90.28%)\n",
      "\n",
      "Train Epoch: 1 [0/48000 (0.00%)]\tLoss: 0.516416\n",
      "Train Epoch: 1 [19200/48000 (40.00%)]\tLoss: 0.207211\n",
      "Train Epoch: 1 [38400/48000 (80.00%)]\tLoss: 0.340304\n",
      "\n",
      "Training set: Accuracy: 44263/48000 (92.21%)\n",
      "Validation set: Accuracy: 11232/12000 (93.60%)\n",
      "\n",
      "Train Epoch: 2 [0/48000 (0.00%)]\tLoss: 0.328751\n",
      "Train Epoch: 2 [19200/48000 (40.00%)]\tLoss: 0.316777\n",
      "Train Epoch: 2 [38400/48000 (80.00%)]\tLoss: 0.336372\n",
      "\n",
      "Training set: Accuracy: 45518/48000 (94.83%)\n",
      "Validation set: Accuracy: 11496/12000 (95.80%)\n",
      "\n",
      "Train Epoch: 3 [0/48000 (0.00%)]\tLoss: 0.219351\n",
      "Train Epoch: 3 [19200/48000 (40.00%)]\tLoss: 0.056791\n",
      "Train Epoch: 3 [38400/48000 (80.00%)]\tLoss: 0.061991\n",
      "\n",
      "Training set: Accuracy: 46097/48000 (96.04%)\n",
      "Validation set: Accuracy: 11563/12000 (96.36%)\n",
      "\n",
      "Train Epoch: 4 [0/48000 (0.00%)]\tLoss: 0.023617\n",
      "Train Epoch: 4 [19200/48000 (40.00%)]\tLoss: 0.144776\n",
      "Train Epoch: 4 [38400/48000 (80.00%)]\tLoss: 0.138505\n",
      "\n",
      "Training set: Accuracy: 46473/48000 (96.82%)\n",
      "Validation set: Accuracy: 11645/12000 (97.04%)\n",
      "\n",
      "Train Epoch: 5 [0/48000 (0.00%)]\tLoss: 0.120009\n",
      "Train Epoch: 5 [19200/48000 (40.00%)]\tLoss: 0.051307\n",
      "Train Epoch: 5 [38400/48000 (80.00%)]\tLoss: 0.067695\n",
      "\n",
      "Training set: Accuracy: 46765/48000 (97.43%)\n",
      "Validation set: Accuracy: 11659/12000 (97.16%)\n",
      "\n",
      "Train Epoch: 6 [0/48000 (0.00%)]\tLoss: 0.041948\n",
      "Train Epoch: 6 [19200/48000 (40.00%)]\tLoss: 0.115152\n",
      "Train Epoch: 6 [38400/48000 (80.00%)]\tLoss: 0.126470\n",
      "\n",
      "Training set: Accuracy: 46888/48000 (97.68%)\n",
      "Validation set: Accuracy: 11737/12000 (97.81%)\n",
      "\n",
      "Train Epoch: 7 [0/48000 (0.00%)]\tLoss: 0.024100\n",
      "Train Epoch: 7 [19200/48000 (40.00%)]\tLoss: 0.080297\n",
      "Train Epoch: 7 [38400/48000 (80.00%)]\tLoss: 0.011328\n",
      "\n",
      "Training set: Accuracy: 46997/48000 (97.91%)\n",
      "Validation set: Accuracy: 11738/12000 (97.82%)\n",
      "\n",
      "Train Epoch: 8 [0/48000 (0.00%)]\tLoss: 0.200716\n",
      "Train Epoch: 8 [19200/48000 (40.00%)]\tLoss: 0.024369\n",
      "Train Epoch: 8 [38400/48000 (80.00%)]\tLoss: 0.220711\n",
      "\n",
      "Training set: Accuracy: 47123/48000 (98.17%)\n",
      "Validation set: Accuracy: 11749/12000 (97.91%)\n",
      "\n",
      "Train Epoch: 9 [0/48000 (0.00%)]\tLoss: 0.013814\n",
      "Train Epoch: 9 [19200/48000 (40.00%)]\tLoss: 0.118231\n",
      "Train Epoch: 9 [38400/48000 (80.00%)]\tLoss: 0.090147\n",
      "\n",
      "Training set: Accuracy: 47193/48000 (98.32%)\n",
      "Validation set: Accuracy: 11754/12000 (97.95%)\n",
      "\n",
      "Train Epoch: 10 [0/48000 (0.00%)]\tLoss: 0.057977\n",
      "Train Epoch: 10 [19200/48000 (40.00%)]\tLoss: 0.023781\n",
      "Train Epoch: 10 [38400/48000 (80.00%)]\tLoss: 0.045531\n",
      "\n",
      "Training set: Accuracy: 47284/48000 (98.51%)\n",
      "Validation set: Accuracy: 11789/12000 (98.24%)\n",
      "\n",
      "Train Epoch: 11 [0/48000 (0.00%)]\tLoss: 0.003574\n",
      "Train Epoch: 11 [19200/48000 (40.00%)]\tLoss: 0.049858\n",
      "Train Epoch: 11 [38400/48000 (80.00%)]\tLoss: 0.037287\n",
      "\n",
      "Training set: Accuracy: 47328/48000 (98.60%)\n",
      "Validation set: Accuracy: 11796/12000 (98.30%)\n",
      "\n",
      "Train Epoch: 12 [0/48000 (0.00%)]\tLoss: 0.037054\n",
      "Train Epoch: 12 [19200/48000 (40.00%)]\tLoss: 0.084335\n",
      "Train Epoch: 12 [38400/48000 (80.00%)]\tLoss: 0.050287\n",
      "\n",
      "Training set: Accuracy: 47348/48000 (98.64%)\n",
      "Validation set: Accuracy: 11790/12000 (98.25%)\n",
      "\n",
      "Train Epoch: 13 [0/48000 (0.00%)]\tLoss: 0.054171\n",
      "Train Epoch: 13 [19200/48000 (40.00%)]\tLoss: 0.005308\n",
      "Train Epoch: 13 [38400/48000 (80.00%)]\tLoss: 0.015637\n",
      "\n",
      "Training set: Accuracy: 47389/48000 (98.73%)\n",
      "Validation set: Accuracy: 11808/12000 (98.40%)\n",
      "\n",
      "Train Epoch: 14 [0/48000 (0.00%)]\tLoss: 0.005319\n",
      "Train Epoch: 14 [19200/48000 (40.00%)]\tLoss: 0.125123\n",
      "Train Epoch: 14 [38400/48000 (80.00%)]\tLoss: 0.047870\n",
      "\n",
      "Training set: Accuracy: 47490/48000 (98.94%)\n",
      "Validation set: Accuracy: 11791/12000 (98.26%)\n",
      "\n",
      "Test set: Accuracy: 9848/10000 (98.48%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network = Model_1().to(device)\n",
    "rlt_const = training(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64815daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/48000 (0.00%)]\tLoss: 2.320723\n",
      "Train Epoch: 0 [19200/48000 (40.00%)]\tLoss: 0.399224\n",
      "Train Epoch: 0 [38400/48000 (80.00%)]\tLoss: 0.139076\n",
      "\n",
      "Training set: Accuracy: 43489/48000 (90.60%)\n",
      "Validation set: Accuracy: 11616/12000 (96.80%)\n",
      "\n",
      "Train Epoch: 1 [0/48000 (0.00%)]\tLoss: 0.028750\n",
      "Train Epoch: 1 [19200/48000 (40.00%)]\tLoss: 0.028404\n",
      "Train Epoch: 1 [38400/48000 (80.00%)]\tLoss: 0.085736\n",
      "\n",
      "Training set: Accuracy: 46788/48000 (97.47%)\n",
      "Validation set: Accuracy: 11760/12000 (98.00%)\n",
      "\n",
      "Train Epoch: 2 [0/48000 (0.00%)]\tLoss: 0.017078\n",
      "Train Epoch: 2 [19200/48000 (40.00%)]\tLoss: 0.176875\n",
      "Train Epoch: 2 [38400/48000 (80.00%)]\tLoss: 0.034385\n",
      "\n",
      "Training set: Accuracy: 47079/48000 (98.08%)\n",
      "Validation set: Accuracy: 11763/12000 (98.03%)\n",
      "\n",
      "Train Epoch: 3 [0/48000 (0.00%)]\tLoss: 0.046184\n",
      "Train Epoch: 3 [19200/48000 (40.00%)]\tLoss: 0.044332\n",
      "Train Epoch: 3 [38400/48000 (80.00%)]\tLoss: 0.111930\n",
      "\n",
      "Training set: Accuracy: 47293/48000 (98.53%)\n",
      "Validation set: Accuracy: 11801/12000 (98.34%)\n",
      "\n",
      "Train Epoch: 4 [0/48000 (0.00%)]\tLoss: 0.015534\n",
      "Train Epoch: 4 [19200/48000 (40.00%)]\tLoss: 0.050992\n",
      "Train Epoch: 4 [38400/48000 (80.00%)]\tLoss: 0.046916\n",
      "\n",
      "Training set: Accuracy: 47458/48000 (98.87%)\n",
      "Validation set: Accuracy: 11818/12000 (98.48%)\n",
      "\n",
      "Train Epoch: 5 [0/48000 (0.00%)]\tLoss: 0.010856\n",
      "Train Epoch: 5 [19200/48000 (40.00%)]\tLoss: 0.045771\n",
      "Train Epoch: 5 [38400/48000 (80.00%)]\tLoss: 0.079960\n",
      "\n",
      "Training set: Accuracy: 47460/48000 (98.88%)\n",
      "Validation set: Accuracy: 11877/12000 (98.97%)\n",
      "\n",
      "Train Epoch: 6 [0/48000 (0.00%)]\tLoss: 0.009314\n",
      "Train Epoch: 6 [19200/48000 (40.00%)]\tLoss: 0.049767\n",
      "Train Epoch: 6 [38400/48000 (80.00%)]\tLoss: 0.067407\n",
      "\n",
      "Training set: Accuracy: 47556/48000 (99.07%)\n",
      "Validation set: Accuracy: 11857/12000 (98.81%)\n",
      "\n",
      "Train Epoch: 7 [0/48000 (0.00%)]\tLoss: 0.007685\n",
      "Train Epoch: 7 [19200/48000 (40.00%)]\tLoss: 0.064280\n",
      "Train Epoch: 7 [38400/48000 (80.00%)]\tLoss: 0.010212\n",
      "\n",
      "Training set: Accuracy: 47613/48000 (99.19%)\n",
      "Validation set: Accuracy: 11862/12000 (98.85%)\n",
      "\n",
      "Train Epoch: 8 [0/48000 (0.00%)]\tLoss: 0.021622\n",
      "Train Epoch: 8 [19200/48000 (40.00%)]\tLoss: 0.015204\n",
      "Train Epoch: 8 [38400/48000 (80.00%)]\tLoss: 0.009260\n",
      "\n",
      "Training set: Accuracy: 47649/48000 (99.27%)\n",
      "Validation set: Accuracy: 11864/12000 (98.87%)\n",
      "\n",
      "Train Epoch: 9 [0/48000 (0.00%)]\tLoss: 0.043549\n",
      "Train Epoch: 9 [19200/48000 (40.00%)]\tLoss: 0.012116\n",
      "Train Epoch: 9 [38400/48000 (80.00%)]\tLoss: 0.010199\n",
      "\n",
      "Training set: Accuracy: 47707/48000 (99.39%)\n",
      "Validation set: Accuracy: 11869/12000 (98.91%)\n",
      "\n",
      "Train Epoch: 10 [0/48000 (0.00%)]\tLoss: 0.001455\n",
      "Train Epoch: 10 [19200/48000 (40.00%)]\tLoss: 0.015868\n",
      "Train Epoch: 10 [38400/48000 (80.00%)]\tLoss: 0.000499\n",
      "\n",
      "Training set: Accuracy: 47732/48000 (99.44%)\n",
      "Validation set: Accuracy: 11875/12000 (98.96%)\n",
      "\n",
      "Train Epoch: 11 [0/48000 (0.00%)]\tLoss: 0.029929\n",
      "Train Epoch: 11 [19200/48000 (40.00%)]\tLoss: 0.018464\n",
      "Train Epoch: 11 [38400/48000 (80.00%)]\tLoss: 0.056356\n",
      "\n",
      "Training set: Accuracy: 47763/48000 (99.51%)\n",
      "Validation set: Accuracy: 11870/12000 (98.92%)\n",
      "\n",
      "Train Epoch: 12 [0/48000 (0.00%)]\tLoss: 0.011684\n",
      "Train Epoch: 12 [19200/48000 (40.00%)]\tLoss: 0.000236\n",
      "Train Epoch: 12 [38400/48000 (80.00%)]\tLoss: 0.011049\n",
      "\n",
      "Training set: Accuracy: 47810/48000 (99.60%)\n",
      "Validation set: Accuracy: 11862/12000 (98.85%)\n",
      "\n",
      "Train Epoch: 13 [0/48000 (0.00%)]\tLoss: 0.007607\n",
      "Train Epoch: 13 [19200/48000 (40.00%)]\tLoss: 0.009107\n",
      "Train Epoch: 13 [38400/48000 (80.00%)]\tLoss: 0.003745\n",
      "\n",
      "Training set: Accuracy: 47804/48000 (99.59%)\n",
      "Validation set: Accuracy: 11867/12000 (98.89%)\n",
      "\n",
      "Train Epoch: 14 [0/48000 (0.00%)]\tLoss: 0.004725\n",
      "Train Epoch: 14 [19200/48000 (40.00%)]\tLoss: 0.001555\n",
      "Train Epoch: 14 [38400/48000 (80.00%)]\tLoss: 0.020369\n",
      "\n",
      "Training set: Accuracy: 47836/48000 (99.66%)\n",
      "Validation set: Accuracy: 11854/12000 (98.78%)\n",
      "\n",
      "Test set: Accuracy: 9885/10000 (98.85%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network = Model_2().to(device)\n",
    "rlt_const = training(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b1471",
   "metadata": {},
   "source": [
    "## 9. 두모델의 성능을 비교하시오"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f8156f",
   "metadata": {},
   "source": [
    "정답)relu 모델이 더 좋다고 볼 수 잇음. 첫 번째 모델보다 더 높은 정확도를 보이고, loss도 더 작기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d1b4448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이서영 2020125046\n"
     ]
    }
   ],
   "source": [
    "print(\"이서영 2020125046\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504227ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
